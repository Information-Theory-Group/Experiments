{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmpc_5mofb9r"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install transformers_stream_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "Ej8PAR2FV7LI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train[:10]\", cache_dir=\"./hf_cache\")\n",
        "\n",
        "# for example in dataset:\n",
        "#     print(\"Q:\", example[\"question\"], \"A:\", example[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2smjskQmvmQ1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
        "# model_name = \"Qwen/Qwen-1_8B-Chat\"\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cWQyUL15c9r0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class TopKSamplerWithEntropy:\n",
        "    def __init__(self, model, tokenizer, k=10, max_length=50):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.k = k\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def compute_entropy(self, probs):\n",
        "        return -torch.sum(probs * probs.log(), dim=-1).item() # H(p)=−∑p(x)logp(x), entropy for the output probability distribution:\n",
        "\n",
        "\n",
        "    def compute_layerwise_entropy(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Compute entropy for each layer's hidden state at the last token position.\n",
        "        Normalize and apply softmax across hidden dimensions.\n",
        "        Here we are computing entropy of that vector as if it were a probablilty distribution.\n",
        "        But hidden states are not probability distributions. But it can still give a useful signal about how focused the layer’s activation is.\n",
        "\n",
        "        Output: a list of entropies — one per layer per step.\n",
        "        \"\"\"\n",
        "        entropies = []\n",
        "        for layer in hidden_states:\n",
        "            last_token_vec = layer[0, -1, :]  # [hidden_dim]\n",
        "            probs = F.softmax(last_token_vec, dim=-1)\n",
        "            entropy = -torch.sum(probs * probs.log()).item()\n",
        "            entropies.append(entropy)\n",
        "        return entropies  # one value per layer\n",
        "\n",
        "    def sample(self, prompt):\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        output_ids = input_ids.clone()\n",
        "\n",
        "        output_entropies = []\n",
        "        layerwise_entropies = []\n",
        "\n",
        "        for _ in range(self.max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(output_ids, output_hidden_states=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                # Output entropy\n",
        "                entropy = self.compute_entropy(probs)\n",
        "                output_entropies.append(entropy)\n",
        "\n",
        "                # Layerwise entropy\n",
        "                hidden_states = outputs.hidden_states  # tuple of [layer_i] each of shape [1, seq_len, hidden_dim]\n",
        "                layer_entropies = self.compute_layerwise_entropy(hidden_states)\n",
        "                layerwise_entropies.append(layer_entropies)\n",
        "\n",
        "                # Top-k sampling\n",
        "                topk_probs, topk_indices = torch.topk(probs, self.k, dim=-1)\n",
        "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
        "                next_token = torch.multinomial(topk_probs, num_samples=1)\n",
        "                next_token_id = topk_indices.gather(-1, next_token)\n",
        "                output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
        "\n",
        "        generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return generated_text, output_entropies, layerwise_entropies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_ftaQ4yUgAIi"
      },
      "outputs": [],
      "source": [
        "sampler = TopKSamplerWithEntropy(model, tokenizer, k=10, max_length=30)\n",
        "\n",
        "for i, example in enumerate(dataset):\n",
        "    print(f\"\\nExample {i + 1}\")\n",
        "    prompt = example[\"question\"]\n",
        "    print(\"Prompt:\", prompt)\n",
        "\n",
        "    generated, output_entropies, layerwise_entropies = sampler.sample(prompt)\n",
        "    print(\"Generated:\", generated)\n",
        "    print(\"Output entropy:\", output_entropies)\n",
        "    print(\"Layerwise entropy (per step):\")\n",
        "    for step_idx, entropies in enumerate(layerwise_entropies):\n",
        "        print(f\"Step {step_idx + 1}: {entropies}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V6PGd-JMg_ko"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def visualize_entropies(prompt, generated_text, output_entropies, layerwise_entropies, save_path=None):\n",
        "    num_layers = len(layerwise_entropies)\n",
        "    num_tokens = len(output_entropies)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [1, 2]})\n",
        "    fig.suptitle(\"Top-k Sampling and Entropy Visualization\", fontsize=16)\n",
        "\n",
        "    # FOR Output entropy over generation steps\n",
        "    axs[0].plot(range(1, num_tokens + 1), output_entropies, marker='o', label=\"Output Entropy\")\n",
        "    axs[0].set_title(\"Token-wise Output Entropy\")\n",
        "    axs[0].set_xlabel(\"Generation Step\")\n",
        "    axs[0].set_ylabel(\"Entropy\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # FOR  Layer-wise entropy heatmap\n",
        "    entropy_matrix = np.array(layerwise_entropies)  # shape: [num_layers, num_tokens]\n",
        "    sns.heatmap(entropy_matrix, ax=axs[1], cmap=\"viridis\", xticklabels=True, yticklabels=True)\n",
        "    axs[1].set_title(\"Layerwise Entropy Heatmap\")\n",
        "    axs[1].set_xlabel(\"Generation Step\")\n",
        "    axs[1].set_ylabel(\"Layer\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzginbs8p5Cm"
      },
      "outputs": [],
      "source": [
        "# TESTING with examples from the dataset\n",
        "example = dataset[0]\n",
        "prompt = example[\"question\"]\n",
        "\n",
        "generated, output_entropies, layerwise_entropies = sampler.sample(prompt)\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated:\", generated)\n",
        "print(\"Output Entropies:\", output_entropies)\n",
        "\n",
        "visualize_entropies(prompt, generated, output_entropies, layerwise_entropies)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
